# Display mode: "mock" for local dev, "eink" on the Pi
HUD_DISPLAY_MODE=mock

# Where mock display saves PNG frames
HUD_MOCK_OUTPUT_DIR=output

# Audio mode: "mock" for local dev, "hardware" on the Pi
HUD_AUDIO_MODE=mock

# Audio format (16kHz mono = Whisper's preferred input)
# HUD_AUDIO_SAMPLE_RATE=16000
# HUD_AUDIO_CHANNELS=1

# Audio input device index or name (run `python -m sounddevice` to list)
# Leave unset for system default; set to device index on headless Pi
# HUD_AUDIO_DEVICE=0

# Where mock audio saves WAV files
# HUD_AUDIO_MOCK_DIR=output/audio

# STT mode: "mock" for local dev, "whisper" for real transcription
HUD_STT_MODE=mock

# Whisper model: "base.en" (default), "tiny.en" (fastest), "small.en" (most accurate)
# English-only (.en) variants are faster and more accurate for English
# HUD_STT_WHISPER_MODEL=base.en

# Whisper prompt: natural sentence with proper nouns to improve recognition
# Include names of shows, movies, or terms Whisper tends to mishear
# HUD_STT_WHISPER_PROMPT=Fringe, Severance, The Expanse, Sonarr, Radarr

# Whisper hotwords: comma-separated words/phrases to boost during transcription
# HUD_STT_WHISPER_HOTWORDS=Fringe, Severance, Expanse, grocery, reminder

# Canned response for mock STT
# HUD_STT_MOCK_RESPONSE=hello world

# TTS mode: "mock" for local dev, "kokoro" for real speech synthesis
HUD_TTS_MODE=mock

# Kokoro voice settings (requires: pip install kokoro-onnx, sudo apt install espeak-ng)
# Uses ONNX Runtime with INT8 quantized model for fast inference on Pi
# Voices: af_heart, af_bella, af_nicole, am_adam, am_michael, bf_emma, bm_george, ...
# Browse: https://huggingface.co/hexgrad/Kokoro-82M
# HUD_TTS_KOKORO_VOICE=af_heart
# HUD_TTS_KOKORO_SPEED=1.0
# HUD_TTS_KOKORO_LANG=a               # "a"/"en-us" = American English, "b"/"en-gb" = British English
# HUD_TTS_KOKORO_MODEL=models/kokoro-v1.0.int8.onnx   # Path to ONNX model file
# HUD_TTS_KOKORO_VOICES=models/voices-v1.0.bin         # Path to voices file

# HUD_TTS_MOCK_DURATION=2.0

# Voice pipeline: enable/disable, record duration after wake word
HUD_VOICE_ENABLED=true
# HUD_VOICE_RECORD_DURATION=5

# Wake word audio feedback (speaks a TTS prompt on wake detection)
# HUD_VOICE_WAKE_FEEDBACK=true

# Startup and deploy announcements (spoken TTS phrases)
# HUD_VOICE_STARTUP_ANNOUNCEMENT=true
# HUD_VOICE_DEPLOY_ANNOUNCEMENT=true

# Voice Activity Detection (dynamic recording — stops on silence)
# HUD_VOICE_VAD_ENABLED=true
# HUD_VAD_SILENCE_THRESHOLD=300           # RMS energy below this = silence
# HUD_VAD_SILENCE_DURATION=2.5            # Seconds of silence before stopping
# HUD_VAD_MIN_DURATION=0.5
# HUD_VAD_MAX_DURATION=15.0
# HUD_VAD_SPEECH_CHUNKS_REQUIRED=3        # Loud chunks needed before silence can trigger stop

# Barge-in: allow interrupting TTS playback with wake word
# HUD_VOICE_BARGEIN_ENABLED=true

# Wake word detection: "mock" for local dev, "oww" for openWakeWord on the Pi
HUD_WAKE_MODE=mock
# HUD_WAKE_MODEL=hey_jarvis
# HUD_WAKE_THRESHOLD=0.5
# HUD_WAKE_MOCK_TRIGGER_AFTER=62

# LLM mode: "mock" for local dev, "claude" for real Anthropic API
HUD_LLM_MODE=mock
# HUD_LLM_MODEL=claude-sonnet-4-5-20250929
# HUD_LLM_MAX_TOKENS=1024
# HUD_LLM_SYSTEM_PROMPT=
# HUD_LLM_MOCK_RESPONSE=This is a mock LLM response.
# HUD_LLM_MAX_HISTORY=10
# HUD_LLM_HISTORY_TTL=300

# Intent recovery: LLM classifies misheard commands and routes to correct feature
# Set to "false" to disable (saves an API call when no feature matches)
# HUD_INTENT_RECOVERY_ENABLED=true

# Grocery list storage path
# HUD_GROCERY_FILE=data/grocery.json

# Reminder storage path and check interval (seconds)
# HUD_REMINDER_FILE=data/reminders.json
# HUD_REMINDER_CHECK_INTERVAL=15

# Refresh interval in seconds (300 = 5 minutes, good for e-ink)
HUD_REFRESH_INTERVAL=300

# Logging
# HUD_LOG_DIR=logs
# HUD_LOG_LEVEL=INFO

# Enphase solar monitoring
# ENPHASE_MODE=mock                # "mock" for local dev, "live" for real gateway
# ENPHASE_HOST=192.168.1.67       # IQ Gateway local IP
#
# Authentication (choose one):
#   Option A (recommended): Auto-generate token from Enlighten credentials
#     The app will auto-generate a JWT, cache it, and refresh before expiry.
# ENPHASE_EMAIL=                   # Enlighten account email
# ENPHASE_PASSWORD=                # Enlighten account password
# ENPHASE_SERIAL=                  # Gateway serial number (see /info endpoint)
#   Option B: Pre-generated JWT token (from Enphase web portal)
# ENPHASE_TOKEN=                   # Paste token here; bypasses auto-generation
#
# ENPHASE_POLL_INTERVAL=600        # Seconds between production polls (default: 600)
# SOLAR_DB_PATH=data/solar.db     # SQLite database for readings
# SOLAR_LATITUDE=                  # For weather API (Open-Meteo)
# SOLAR_LONGITUDE=                 # For weather API (Open-Meteo)

# Sonarr (TV shows) — opt-in: set SONARR_MODE to enable
# SONARR_MODE=                    # "" (disabled) | "mock" | "live"
# SONARR_URL=http://localhost:8989
# SONARR_API_KEY=                 # Sonarr Settings → General → API Key

# Radarr (Movies) — opt-in: set RADARR_MODE to enable
# RADARR_MODE=                    # "" (disabled) | "mock" | "live"
# RADARR_URL=http://localhost:7878
# RADARR_API_KEY=                 # Radarr Settings → General → API Key

# Media disambiguation timeout (seconds) — how long "yes/no/next" stays active
# HUD_MEDIA_DISAMBIGUATION_TTL=60

# Voice transaction telemetry (SQLite)
# HUD_TELEMETRY_ENABLED=true
# HUD_TELEMETRY_DB_PATH=data/telemetry.db
# HUD_TELEMETRY_MAX_SIZE_MB=10240         # 10GB rollover limit

# Anthropic API for intent parsing (Phase 6)
# ANTHROPIC_API_KEY=
